---
layout: post
title:  "Classification using Naive Bayes"
date:   2017-04-08 10:55:05 +0900
categories: machine-learning
tags: [programming, javascript, node.js, framework, server]
published: true
writer: pjt3591oo
count: 125
---

<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_SVG"></script>

머신러닝은 인공지능의 한 분야로, 컴퓨터가 학습할 수 있도록 하는 알고리즘과 기술을 개발하는 분야이다. 예를 들어 추천 시스템, 이메일 추천 필터링 등을 할 수 있도록 훈련을 할 수 있다.
머신러닝은 크게 **Supervised learning(지도학습)**, **Unsupervised Learning(비지도 학습)**, **Reinforcement learning(강화학습)**으로 나뉜다.

```
1. 지도학습 : 훈련 데이터(Training Data)로부터 하나의 함수(결과)를 도출
    ex) Support vector machine, Hidden Markov model, Regression, Neural network, Naive Bayes Classification

2. 비지도 학습 : 데이터가 어떻게 구성되있는지 모른다.
                지도학습, 강화학습과 달리 데이터의 목표가 없다.
    ex) Clustering, Independent Component Analysis

3. 강화학습 : 환경을 탐색하는 에이전트가 현재의 상태를 인식하여 어떤 행동을 취함
             에이전트는 환경으로 부터 포상을 받게된다.
             에이전트는 누적될 포상을 최대한 하는 방법을 찾는다.

* 지도학습 vs 강화학습
  지도학습과 강화학습의 공통점으로 둘다 목표치가 존재한다. 그러나 지도학습은 이미 알고있는 데이터를 기반으로 원하지 않는 행동을 명시적으로 수정하지만 강화학습은 알려지지 않는 영역을 찾음으로써 목표치에 도달한다.
```

Naive Bayes Classification 개념
---
지도학습 중에서 특성들 사이의 독립을 가정하는  Naive Bayes Classification에 대해서 알아보고자 한다.
우선, 특성들 사이의 **독립**을 가정한다라는 말에 대해서 알아보자.
예를 들어 특정 과일을 사과로 분류 가능하게 하는 특성들(둥글다, 빨갛다, 지름 10Cm)은 베이즈 정리에서는 각각의 특성이 서로 연관이 없다.
색상은 모양이나 크기에 전혀 영향을 미치지 않는것을 의미한다.
즉, 이것을 특성들 사이의 `독립성`이라고 말한다. 또한 이것을 `조건부 확률 모델`이라고 말한다.

* 베이즈 장점

    * 효율적인 훈련
    * Training에 필요한 data가 적다.(text 1만줄 정도만으로도 분류기로서의 제 역할이 가능하다.)
    * 간단한 디자인과 단순한 가정에도 불구하고, 복잡한 실제 상황에서 잘 작동.

조건부 확률로 분류하기
---
나이브 베이즈는 조건부 활률 모델이다. 분류될 인스턴스들은 N개의 특성을 나타내는
벡터 x =  ($${x_1},...,{x_n}$$)로 표현되며, 나이브 베이즈 분류기는 이 벡터를 이용하여 k개의 가능한 확률적 결과들을 다음과 같이 할당한다.
  $$p({C_k}|{x_1},...,{x_n})$$

우리는 p(x\|c)를 알고있는 상태에서 p(c\|x)를 알기 원한다. 이것을 찾기 위해 다음과 같은 공식을 사용한다.
  $$p({C_k}|X)=$$ $$p({C_k}) p(X |{C_k}) \above 1pt p(X)$$

우리는 나이브 베이즈 이론을 사용하여 분류를 할때 training data를 필요로 한다고 했다. 이때 위에서 예시를 든 과일 분류기를 통해 사과를 분류하는 분류기가 있다고 가정을 해보자 이때 training data로 필요한 데이터는 사과인 것들의 특성(p1)들과 사과가 아닌 과일들의 특성(p2)들이 필요로 한다. 그리고 이 특성들이 얼마나 반영이 되는지 확률을 계산하는데 각각의 확률 결과가 p1 > p2일 경우 해당 과일은 사과로 분류가 되고 p1 < p2일 경우 해당 과일은 사과가 아닌 과일로 분류가 된다.

이것을 좀더 간결한 식으로 표현을 해보면 아래와 같다
$$p({c_i}|x,y)=$$ $$p({c,y|{c_i}}) p({c_i}) \above 1pt p(x,y)$$
라고 정의를 해두자.

만약에
$$p({c_1}|x,y)>p({c_2}|x,y)$$
이면 분류 항목 c1에 속한다.

만약에
$$p({c_1}|x,y)<p({c_2}|x,y)$$
이면 분류 항목 c2에 속한다.


쇼핑몰 분류
---
위의 이론을 바탕으로 크롤러에 적용을 시킬 **분류기**를 만들어보자.

1. 첫번째로 training data를 만든다. training data를 만들기 위해 약 60여개의 쇼핑몰과 20~30여개의 비 쇼핑몰 사이트의 text들을 파싱을 한다. 이후 파싱 결과를 LDA를 통해 각각의 페이지에서 가장 많이 나온 상위 text들을 뽑아낸다. 이후 형태소 분석을 통해 더미 데이터는 필터를 해준 후 저장을 한다.

2. 이제 크롤러는 검색엔진을 통해 열심히 페이지를 수집을 한다. 이후 해당 페이지에서 text들을 파싱하여 나이브 베이즈를 적용하여 쇼핑몰 분류기에 넣어준다. 이때 분류기는 쇼핑몰에 나온 단어들과 쇼핑몰이 아닌 페이지에 나온 상위 단어들과 확률계산을 한다.

3. 확률이 큰쪽으로 결과를 도출한다. 결과에 있어서 페이지를 크게 3개의 종류로 나누어 보았다. 쇼핑몰 페이지, 패션 기사, 일반 페이지로 나누었다. 3번까지만 적용을 했을경우 쇼핑몰 페이지와 패션 기사를 거의 구분을 하지 못하는 오류가 있었다.

4. 확률을 계산하는 과정에서 쇼핑몰의 특성을 조금더 반영을 해보았다. 바로 가격표시이다. 초기에 3번까지만 적용을 했을때는 약 40%정도 오차율을 가지고 있었다. 하지만 가격이라는 특성을 반영을 한후 오차율은 상당히 많이 줄었음을 확인할 수 있었다.(쇼핑몰은 대부분 가격이라는 정보를 띄어준다.)
`/\d.+\,\d+/g` 정규식을 통해 숫자,숫자 형식의 text를 찾고 그 갯수만큼 각각의 p1, p2 결과에 반영을 해주었다. 또한 가격표시가 0이라면 이는 쇼핑몰이 아니라고 확신을 한 후 무조건 적으로 쇼핑몰이 아니라고 결과를 도출한다.

> 결론적으로 나이브 베이즈 정리는 분류 되어야 할 대상의 모델이 어느 조건부 확률 모델에 가까운지 계산하는 것입니다. 그렇기 때문에 간단한 통계만 가지고 쉽게 분류가 가능합니다.

